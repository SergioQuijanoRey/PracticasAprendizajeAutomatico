\documentclass[11pt]{article}

% Paquetes
%===================================================================================================

% Establecemos los márgenes
\usepackage[a4paper, margin=1in]{geometry}

% Separacion entre parrafos
\setlength{\parskip}{1em}

% Paquete para incluir codigo
\usepackage{listings}

% Paquete para incluir imagenes
\usepackage{graphicx}
\graphicspath{ {./Imagenes/} }

% Para fijar las imagenes en la posicion deseada
\usepackage{float}

% Para que el codigo acepte caracteres en utf8
\lstset{literate=
  {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
  {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
  {à}{{\`a}}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
  {À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
  {ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
  {Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
  {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
  {Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
  {ã}{{\~a}}1 {ẽ}{{\~e}}1 {ĩ}{{\~i}}1 {õ}{{\~o}}1 {ũ}{{\~u}}1
  {Ã}{{\~A}}1 {Ẽ}{{\~E}}1 {Ĩ}{{\~I}}1 {Õ}{{\~O}}1 {Ũ}{{\~U}}1
  {œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
  {ű}{{\H{u}}}1 {Ű}{{\H{U}}}1 {ő}{{\H{o}}}1 {Ő}{{\H{O}}}1
  {ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
  {€}{{\euro}}1 {£}{{\pounds}}1 {«}{{\guillemotleft}}1
  {»}{{\guillemotright}}1 {ñ}{{\~n}}1 {Ñ}{{\~N}}1 {¿}{{?`}}1 {¡}{{!`}}1
}

% Para que no se salgan las lineas de codigo
\lstset{breaklines=true}

% Para que los metadatos que escribe latex esten en español
\usepackage[spanish]{babel}

% Para la bibliografia
% Sin esto, los enlaces de la bibliografia dan un error de compilacion
\usepackage{url}

% Para mostrar graficas de dos imagenes, cada una con su caption, y con un caption comun
\usepackage{subcaption}

% Simbolo de los numeros reales
\usepackage{amssymb}

% Para que los codigos tengan una fuente distinta
\usepackage{courier}

\lstdefinestyle{CustomStyle}{
  language=Python,
  numbers=left,
  stepnumber=1,
  numbersep=10pt,
  tabsize=4,
  showspaces=false,
  showstringspaces=false
  basicstyle=\tiny\ttfamily,
}

% Para incluir tablas en csv
\usepackage{csvsimple}

% Para referenciar secciones usando el nombre de las secciones
\usepackage{nameref}

% Para enumerados dentro de enumerados
\usepackage{enumitem}

% Para mejores tablas
\usepackage{tabularx}

% Para poder tener el mismo identificador en dos tablas separadas
\usepackage{caption}

% Mostrar la página de las referencias en el indice del documento
\usepackage[nottoc,numbib]{tocbibind}

% Metadatos del documento
%===================================================================================================
\title{
    {Aprendizaje Automático - Tercera Práctica}\\
    {Dos caso de uso reales}\\  % TODO -- repensar el titulo de la practica
    {Modelos Lineales}
}

\author{
    {Sergio Quijano Rey - 72103503k}\\
    {4º Doble Grado Ingeniería Informática y Matemáticas}\\
    {sergioquijano@correo.ugr.es}
}

\date{\today}

% Separacion entre parrafos
\setlength{\parskip}{1em}

% Contenido del documento
%===================================================================================================
\begin{document}

% Portada del documento
\maketitle
\pagebreak

% Indice de contenidos
\tableofcontents

% Lista de figuras
\listoffigures

% Lista de tablas
\listoftables

\pagebreak

\section{Problema de regresión}

Los superconductores tienen la interesante propiedad de poder lograr resistencias al paso de la corriente muy cercanas a $0\Omega$. Sin embargo, esto solo ocurre cuando están por debajo de la temperatura crítica para este fenómeno, denotada como $T_c$.

Un superconductor con un un valor de $T_c$ muy bajo no resultaría práctico en aplicaciones de ingeniería, pues para aprovechar sus propiedades interesantes debería realizarse un proceso de enfriamiento que potencialmente consumiría mucha energía. Por tanto, es interesante conocer los valores de $T_c$ de los superconductores, para determinar si es viable o no su aplicación en distintos problemas.

No existe ningún modelo teórico para predecir el valor de $T_c$ de nuevos superconductores, por tanto es interesante plantear un modelo de regresión de aprendizaje automático para predecir dicho valor de $T_c$ \cite{original_paper_reg:paper}.

\subsection{Exploración del problema}

\subsubsection{Descripción del problema}

Disponemos de dos archivos, \lstinline{train.csv} y \lstinline{unique_m.csv}. Este último archivo contiene las fórmulas químicas desglosadas de los superconductores con los que trabajamos. \lstinline{train.csv} contiene 81 características de los superconductores, y el valor de $T_c$ que queremos predecir.

En el propio paper \cite{original_paper_reg:paper} que se encuentra en la página del dataset con el que trabajamos, de UCI, se explica el tratamiento de los datos. En dicha sección, se detalla el proceso de extracción de las 81 características. A partir de las fórmulas codificadas en \lstinline{unique_m.csv}, se extraen propiedades de los átomos que forman las moléculas. Al ser moléculas con más de un átomo, se toman estadísticos de las propiedades. Estas propiedades y estadísticos se detallan en \emph{\ref{descripcion_caracteristicas}. \nameref{descripcion_caracteristicas}}.

Por tanto, no parece factible que seamos capaces de obtener, a partir de conocimiento experto del problema, más \emph{features}, de más alto nivel a ser posible, que resulten útiles para resolver el problema. Consecuentemente, no usaremos la información que nos pueda proporcionar \lstinline{unique_m.csv}, ignorando este \emph{dataset} por completo.

En el mismo paper, el autor comenta: \emph{"We take an entirely data-driven approach"}. Por lo tanto, esto junto a nuestra falta de conocimiento sobre el problema, justifica que usemos ténicas estadísticas para establecer el conjunto de características a emplear (principalmente \emph{PCA}), y una ténica como \emph{cross-validation} para seleccionar el modelo a emplear, las transformaciones sobre los datos y distintos parámetros referentes al modelo escogido.

\pagebreak

\subsubsection{Problema a resolver}

Queremos aprender una función objetivo de la forma:

$$f: \mathcal{X} \rightarrow \mathcal{Y}$$

donde $\mathcal{X}$ es el conjunto real de dimensión 81 (las 81 características de las que disponemos), e $\mathcal{Y}$ son valores reales, en el intervalo $[0, \infty]$. Como comentaremos en \emph{\ref{descripcion_caracteristicas}. \nameref{descripcion_caracteristicas}}, la unidad de medida de la temperatura son los Kelvin, y por tanto, tenemos una cota inferior de esta variable real.

Más adelante realizaremos transformaciones sobre el conjunto de datos original, por lo tanto, pasaremos de aprender un $f: \mathcal{X} \rightarrow \mathcal{Y}$ a aprender un $f: \hat{\mathcal{X}} \rightarrow \mathcal{Y}$, donde $\hat{\mathcal{X}}$ tendrá otra dimensión.

Por tanto quedan claros los elementos de un problema de regresión, queremos encontrar una función $g: \hat{\mathcal{X}} \rightarrow \mathcal{Y}$ de forma que $\forall x \in \hat{\mathcal{X}}, g(x) \approx f(x)$.

\subsubsection{Descripción de las características} \label{descripcion_caracteristicas}

De nuevo, en el paper original \cite{original_paper_reg:paper} se describe el proceso de extracción de características, que pasamos a resumir brevemente.

Se parte de las siguientes propiedades de los átomos que componen las moléculas de los superconductores:

\begin{table}[H]
\begin{tabularx}{\textwidth}{|X|X|}
    \hline
    \textbf{Variable} & \textbf{Descripción} \\
    \hline
    Masa atómica & Masa total del protón y neutrón en reposo \\
    Energía de primera ionización & Energía necesaria para eliminar una valencia del electrón \\
    Radio Atómico & Radio atómico \\
    Densidad & Densidad a una temperatura y presión estándar \\
    Afinidad del electrón & Energía necesaria para añadir un electrón a un átomo neutro \\
    Calor de fusión & Energía necesaria para pasar de estado sólido a líquido sin cambio de temperatura \\
    Conductividad térmica &  Coeficientes de conductividad térmica $\kappa$ \\
    Valencia & Número típico de enlaces químicos formados por el elemento \\
    \hline
\end{tabularx}
\caption{Propiedades de los elementos usadas para crear las \emph{features} }
\end{table}

Las \emph{features} más importantes a la hora de predecir $T_c$ son aquellas basadas en la \emph{thermal conductivity}, \emph{atomic radius}, \emph{valence}, \emph{electron affinity}, y \emph{atomic mass} \cite{original_paper_reg:paper}. Con esto podríamos pensar en descartar el resto de \emph{features} que no se basen en las anteriores, sin embargo, no tenemos el conocimiento suficiente sobre el problema para realizar este descarte con confianza, delegando esta decisión a la técnica \emph{Principal Componente Analysis} que más adelante desarrollaremos.

A partir de esto, se calculan las siguientes \emph{features}. Por cada material, en base a sus moléculas, se calculan los siguientes estadísticos por cada \emph{feature} mostrada en la anterior tabla y por cada átomo de la molécula:


\begin{itemize}
    \item Media
    \item Media ponderada
    \item Media geométrica
    \item Media geométrica ponderada
    \item Entropía
    \item Entropía ponderada
    \item Rango
    \item Rango ponderado
    \item Desviación estándar
    \item Desviación estándar ponderada
\end{itemize}

Algo importante a destacar es que la unidad de temperatura para $T_c$ es \emph{Kelvin}, por lo que esta variable estará acotada inferiormente por cero. La temperatura ambiente en kelvin está en torno a los $298K$, por lo tanto, valores cercanos a esta referencia serán los más interesantes a la hora de escoger o no un material como superconductor.

Las fórmulas para cada estadístico se pueden consultar en el ya mencionado \emph{paper} \cite{original_paper_reg:paper}. Notar que tenemos siempre el estadístico y su versión ponderada.

Teniendo 8 variables, y 10 estadísticas por cada variable, llegamos a 80 características en el dataset. La característica que falta para llegar a las 81, es el número de elementos que compone la molécula del superconductor.

\subsubsection{Exploración del \emph{Dataset}}

Antes de empezar a explorar los datos del problema, separamos el conjunto de \emph{training} y de \emph{test}. No queremos saber nada sobre el \emph{test\_dataset} durante esta exploración de los datos, para evitar caer en el \emph{data snooping}. Esta separación de los datos la realizamos con la función \lstinline{split_data} en la que hacemos:

\begin{lstlisting}[language=Python]
    df_test, df_train = train_test_split(df, test_size = test_percentage, shuffle = True, stratify = None)
\end{lstlisting}

Notar que estamos mezclando los datos pues \lstinline{shuffle = True}, con ello, y teniendo en cuenta que disponemos de muchos datos, queremos tener una muestra de entrenamiento representativa de los datos. Por ejemplo, no queremos que tengamos desbalanceos en la variable de salida, es decir, que en \emph{train} tengamos filas con valores de $T_c$ bajo, mientras que en \emph{test} tengamos filas con valores de $T_c$ altos, o viceversa. Como no tenemos clases, y al tener una cantidad tan grande de datos, no hacemos \emph{stratify != None}. Confiamos en que la mezcla aleatoria haga que nuestras muestras sean representativas y balanceadas, en el sentido que ya se ha especificado.

Partimos de un \emph{dataset} con 21263 ejemplos. Al separar en \emph{train} y \emph{test}, nos quedamos con 17010 y 4253 ejemplos, respectivamente.

Con la función \lstinline{explore_training_set} hacemos una pequeña exploración estadística de los datos, en la que mostramos una tabla con las estadísticas de las columnas. Dicha tabla con el análisis descriptivo de los atributos del conjunto de entrenamiento se muestra en \emph{Tabla \ref{Tabla con los estadísticos de las features}} (tabla que se presenta en dos partes, debido a la gran extensión):

\begin{table}[H]
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
    \textbf{name}                             &  \textbf{mean}&       \textbf{median}&           \textbf{var}&          \textbf{std}&         \textbf{min}&           \textbf{max}&          \textbf{p25}&          \textbf{p75}  \\
\hline
number\_of\_elements               &     4.11&     4.00&  2.08e+0&     1.44&    1.00&      9.00&     3.00&     5.00 \\
mean\_atomic\_mass                 &    87.42&    84.78&  8.80e+2&    29.67&    6.94&    208.98&    72.38&   100.35 \\
wtd\_mean\_atomic\_mass             &    72.95&    60.84&  1.12e+3&    33.56&    6.42&    208.98&    52.07&    86.07 \\
gmean\_atomic\_mass                &    71.17&    66.36&  9.62e+2&    31.02&    5.32&    208.98&    57.78&    78.11 \\
wtd\_gmean\_atomic\_mass            &    58.54&    39.93&  1.34e+3&    36.69&    1.96&    208.98&    35.18&    73.05 \\
entropy\_atomic\_mass              &     1.16&     1.19&  1.34e-1&     0.36&    0.00&      1.98&     0.96&     1.44 \\
wtd\_entropy\_atomic\_mass          &     1.06&     1.14&  1.62e-1&     0.40&    0.00&      1.95&     0.76&     1.35 \\
range\_atomic\_mass                &   115.39&   122.90&  2.98e+3&    54.64&    0.00&    207.97&    78.09&   153.96 \\
wtd\_range\_atomic\_mass            &    33.20&    26.52&  7.33e+2&    27.07&    0.00&    205.58&    16.73&    38.33 \\
std\_atomic\_mass                  &    44.31&    45.02&  4.02e+2&    20.05&    0.00&    101.01&    32.89&    58.97 \\
wtd\_std\_atomic\_mass              &    41.33&    44.27&  3.99e+2&    19.98&    0.00&    101.01&    28.53&    53.58 \\
mean\_fie                         &   770.51&   765.75&  7.76e+3&    88.11&  502.50&   1313.10&   723.74&   797.15 \\
wtd\_mean\_fie                     &   870.52&   889.69&  2.04e+4&   142.98&  502.50&   1348.02&   739.28&  1003.97 \\
gmean\_fie                        &   738.37&   728.82&  6.23e+3&    78.95&  502.50&   1313.10&   692.54&   766.46 \\
wtd\_gmean\_fie                    &   832.96&   855.51&  1.43e+4&   119.63&  502.50&   1327.59&   720.64&   937.55 \\
entropy\_fie                      &     1.29&     1.35&  1.46e-1&     0.38&    0.00&      2.15&     1.08&     1.55 \\
wtd\_entropy\_fie                  &     0.92&     0.91&  1.12e-1&     0.33&    0.00&      2.03&     0.75&     1.06 \\
range\_fie                        &   572.06&   764.10&  9.62e+4&   310.24&    0.00&   1304.50&   259.10&   810.60 \\
wtd\_range\_fie                    &   482.65&   508.21&  5.03e+4&   224.47&    0.00&   1251.85&   290.90&   690.55 \\
std\_fie                          &   215.56&   266.29&  1.21e+4&   110.16&    0.00&    499.67&   113.56&   297.52 \\
wtd\_std\_fie                      &   223.66&   258.10&  1.63e+4&   127.88&    0.00&    477.81&    92.64&   342.60 \\
mean\_atomic\_radius               &   157.85&   160.25&  4.09e+2&    20.24&   48.00&    253.00&   149.00&   169.80 \\
wtd\_mean\_atomic\_radius           &   134.77&   126.02&  8.30e+2&    28.81&   48.00&    253.00&   112.13&   158.38 \\
gmean\_atomic\_radius              &   144.33&   142.80&  4.91e+2&    22.16&   48.00&    253.00&   133.54&   155.93 \\
wtd\_gmean\_atomic\_radius          &   121.07&   113.27&  1.28e+3&    35.82&   48.00&    253.00&    89.22&   151.06 \\
entropy\_atomic\_radius            &     1.26&     1.32&  1.41e-1&     0.37&    0.00&      2.14&     1.06&     1.51 \\
wtd\_entropy\_atomic\_radius        &     1.12&     1.24&  1.66e-1&     0.40&    0.00&      1.90&     0.84&     1.42 \\
range\_atomic\_radius              &   139.13&   171.00&  4.53e+3&    67.34&    0.00&    256.00&    80.00&   205.00 \\
wtd\_range\_atomic\_radius          &    51.41&    43.04&  1.23e+3&    35.12&    0.00&    240.16&    28.53&    60.57 \\
std\_atomic\_radius                &    51.54&    58.66&  5.25e+2&    22.92&    0.00&    115.50&    35.00&    69.42 \\
wtd\_std\_atomic\_radius            &    52.26&    59.74&  6.40e+2&    25.31&    0.00&     97.14&    31.82&    73.66 \\
mean\_Density                     &  6115.33&  5329.08&  8.16e+6&  2858.00&    1.42&  22590.00&  4506.75&  6769.93 \\
wtd\_mean\_Density                 &  5278.72&  4386.11&  1.05e+7&  3240.74&    1.42&  22590.00&  2998.57&  6422.80 \\
gmean\_Density                    &  3464.31&  1339.97&  1.37e+7&  3711.86&    1.42&  22590.00&   883.11&  5802.35 \\
wtd\_gmean\_Density                &  3126.70&  1525.86&  1.59e+7&  3991.46&    0.68&  22590.00&    66.76&  5763.29 \\
entropy\_Density                  &     1.07&     1.09&  1.18e-1&     0.34&    0.00&      1.95&     0.90&     1.32 \\
wtd\_entropy\_Density              &     0.85&     0.88&  1.03e-1&     0.32&    0.00&      1.70&     0.68&     1.07 \\
range\_Density                    &  8672.52&  8958.57&  1.69e+7&  4118.33&    0.00&  22588.57&  6648.00&  9778.57 \\
wtd\_range\_Density                &  2914.45&  2082.95&  5.86e+6&  2421.24&    0.00&  22434.16&  1659.70&  3427.42 \\
std\_Density                      &  3419.54&  3294.07&  2.83e+6&  1682.52&    0.00&  10724.37&  2819.49&  4004.27 \\
wtd\_std\_Density                  &  3318.18&  3623.83&  2.61e+6&  1617.33&    0.00&  10410.93&  2564.34&  3956.79 \\
mean\_ElectronAffinity            &    77.04&    73.10&  7.76e+2&    27.86&    1.50&    326.10&    62.09&    85.85 \\
wtd\_mean\_ElectronAffinity        &    92.77&   102.73&  1.04e+3&    32.35&    1.50&    326.10&    73.39&   110.73 \\
gmean\_ElectronAffinity           &    54.49&    51.53&  8.47e+2&    29.11&    1.50&    326.10&    33.70&    67.57 \\
wtd\_gmean\_ElectronAffinity       &    72.42&    73.08&  1.00e+3&    31.70&    1.50&    326.10&    50.87&    89.96 \\
entropy\_ElectronAffinity         &     1.06&     1.13&  1.18e-1&     0.34&    0.00&      1.76&     0.87&     1.34 \\
wtd\_entropy\_ElectronAffinity     &     0.77&     0.78&  8.25e-2&     0.28&    0.00&      1.67&     0.65&     0.87 \\
range\_ElectronAffinity           &   121.03&   127.05&  3.50e+3&    59.19&    0.00&    349.00&    86.10&   138.63 \\
wtd\_range\_ElectronAffinity       &    59.32&    71.12&  8.29e+2&    28.80&    0.00&    218.69&    33.99&    76.70 \\
    \hline
    \end{tabular}
    }
    \caption{Exploración estadística de los atributos del conjunto de entrenamiento, parte 1}
    \label{Tabla con los estadísticos de las features}
\end{table}

\begin{table}[H]
\ContinuedFloat
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
    \textbf{name}                             &    \textbf{mean}&       \textbf{median}&           \textbf{var}&          \textbf{std}&         \textbf{min}&           \textbf{max}&          \textbf{p25}&          \textbf{p75}  \\
\hline
    std\_ElectronAffinity             &    49.01&    51.12&  4.81e+2&    21.94&    0.00&    162.89&    38.43&    56.52 \\
wtd\_std\_ElectronAffinity            &   44.50&    48.16&  4.23e+2&    20.58&    0.00&    169.07&    33.34&    53.43 \\
mean\_FusionHeat                      &  14.32&     9.33&  1.28e+2&    11.31&    0.22&    105.00&     7.58&    17.22 \\
wtd\_mean\_FusionHeat                 &   13.89&     8.41&  2.04e+2&    14.30&    0.22&    105.00&     5.05&    18.54 \\
gmean\_FusionHeat                     &  10.13&     5.27&  1.01e+2&    10.07&    0.22&    105.00&     4.11&    13.59 \\
wtd\_gmean\_FusionHeat                &   10.16&     4.96&  1.72e+2&    13.14&    0.22&    105.00&     1.32&    16.42 \\
entropy\_FusionHeat                   &   1.09&     1.11&  1.42e-1&     0.37&    0.00&      2.03&     0.82&     1.37 \\
wtd\_entropy\_FusionHeat              &    0.91&     0.99&  1.38e-1&     0.37&    0.00&      1.74&     0.66&     1.15 \\
range\_FusionHeat                     &  21.21&    12.87&  4.19e+2&    20.47&    0.00&    104.77&    12.87&    23.54 \\
wtd\_range\_FusionHeat                &    8.25&     3.45&  1.31e+2&    11.45&    0.00&    102.38&     2.34&    10.49 \\
std\_FusionHeat                       &   8.35&     4.94&  7.60e+1&     8.72&    0.00&     51.63&     4.26&     9.10 \\
wtd\_std\_FusionHeat                  &    7.74&     5.51&  5.36e+1&     7.32&    0.00&     51.68&     4.60&     8.02 \\
mean\_ThermalConductivity             &  89.48&    96.17&  1.48e+3&    38.57&    0.02&    332.50&    60.50&   111.00 \\
wtd\_mean\_ThermalConductivity        &   81.57&    73.55&  2.10e+3&    45.87&    0.02&    406.96&    53.77&    99.04 \\
gmean\_ThermalConductivity            &  29.80&    14.28&  1.16e+3&    34.08&    0.02&    317.88&     8.33&    41.73 \\
wtd\_gmean\_ThermalConductivity       &   27.32&     6.11&  1.62e+3&    40.32&    0.02&    376.03&     1.08&    47.07 \\
entropy\_ThermalConductivity          &   0.72&     0.73&  1.05e-1&     0.32&    0.00&      1.63&     0.45&     0.95 \\
wtd\_entropy\_ThermalConductivity     &    0.53&     0.54&  1.00e-1&     0.31&    0.00&      1.61&     0.24&     0.77 \\
range\_ThermalConductivity            & 250.06&   399.48&  2.52e+4&   158.79&    0.00&    429.97&    86.00&   399.97 \\
wtd\_range\_ThermalConductivity       &   62.11&    56.47&  1.89e+3&    43.56&    0.00&    401.44&    29.25&    91.93 \\
std\_ThermalConductivity              &  98.60&   134.63&  3.61e+3&    60.15&    0.00&    214.98&    37.55&   153.51 \\
wtd\_std\_ThermalConductivity         &   95.98&   113.36&  4.07e+3&    63.81&    0.00&    213.30&    31.89&   162.66 \\
mean\_Valence                         &   3.20&     2.83&  1.09e+0&     1.04&    1.00&      7.00&     2.33&     4.00 \\
wtd\_mean\_Valence                    &    3.16&     2.63&  1.42e+0&     1.19&    1.00&      7.00&     2.11&     4.05 \\
gmean\_Valence                        &   3.06&     2.61&  1.10e+0&     1.04&    1.00&      7.00&     2.28&     3.77 \\
wtd\_gmean\_Valence                   &    3.06&     2.43&  1.39e+0&     1.17&    1.00&      7.00&     2.09&     3.94 \\
entropy\_Valence                      &   1.29&     1.36&  1.55e-1&     0.39&    0.00&      2.14&     1.06&     1.58 \\
wtd\_entropy\_Valence                 &    1.05&     1.16&  1.45e-1&     0.38&    0.00&      1.94&     0.76&     1.33 \\
range\_Valence                        &   2.04&     2.00&  1.55e+0&     1.24&    0.00&      6.00&     1.00&     3.00 \\
wtd\_range\_Valence                   &    1.48&     1.06&  9.68e-1&     0.98&    0.00&      6.99&     0.91&     1.92 \\
std\_Valence                          &   0.84&     0.80&  2.37e-1&     0.48&    0.00&      3.00&     0.47&     1.21 \\
wtd\_std\_Valence                     &    0.67&     0.50&  2.09e-1&     0.45&    0.00&      3.00&     0.30&     1.02 \\
critical\_temp                        &  34.37&    20.00&  1.17e+3&    34.25&    0.00&    185.00&     5.30&    63.00 \\
    \hline
    \end{tabular}
    }
    \caption{Exploración estadística de los atributos del conjunto de entrenamiento, parte 2}
    \label{Tabla con los estadísticos de las features}
\end{table}

No mostramos el valor \emph{missing values}, porque en todos los casos son cero, así que no tenemos que preocuparnos de cómo afrontar este problema. Tampoco mostramos el valor de \emph{type}. Todos los valores son \emph{float64}, salvo \emph{number\_of\_elements}, \emph{range\_atomic\_radius} y \emph{range\_Valence}, que son \emph{int64}

La tabla deja claro que los rangos de las variables son muy dispares, así como las desviaciones típicas. Por ejemplo, wtd\_mean\_ThermalConductivity toma un rango de valores que va desde 0 hasta 406.96, mientas que por ejemplo entropy\_ThermalConductivity va desde 0 hasta 1.63. Lo mismo se puede decir de las desviaciones típicas. Muchos algoritmos y modelos son sensibles a rangos de valores dispares entre distintas características. Otros directamente esperan que se siga una distribución parecida a una normal para tener un comportamiento decente. Por tanto, y como no es perjudicial realizar una estandarización, queda justificada la posterior estandarización que vamos a llevar a cabo.

Otro elemento a tener en cuenta es que todas las variables son reales o enteras, y por tanto, tampoco es necesaria una técnica como \emph{one hot encoding} para codificar variables categóricas.

Con estos datos, podemos mostrar \emph{boxplots} de las variables, pero tampoco extraeríamos demasiada información, pues más tarde vamos a estandarizar los datos, como ya hemos comentado, y además, vamos a eliminar los \emph{outliers}.Sin embargo, la variable de salida \emph{critical\_temp} no va a ser estandarizada ni eliminados los \emph{outliers} asociados a esta columna. Mostramos su \emph{boxplot}:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{output_var_boxplot}
    \caption{Boxplot de la temperatura crítica}
\end{figure}

La caja del gráfico muestran los extremos de los extremos que fijan el percentil 25 y 75. Por tanto, podemos ver que nuestros datos de entrada están muy acumulados en valores de salida bajos. Es decir, la mayoría de datos con los que trabajamos están asociados a superconductores con un $T_c$ bajo, y por tanto menos interesantes. Esto mismo se comenta en el paper original, en la figura 4 \cite{original_paper_reg:paper}.

Por tanto, debemos preservar estos \emph{outliers} en la variable de salida, pues son precisamente los datos que nos interesa predecir. Podríamos intentar solucionar este desbalanceo eliminando datos en la parte de mayor acumulación (datos de baja temperatura $T_c$). Sin embargo, a vista de lo desplazada que está la caja del gráfico, eliminaríamos demasiados datos, con lo que seguramente no mejoraríamos el rendimiento de la función aprendida. Además, nuestro objetivo no es aprender bien la función para superconductores con $T_c$ alto (aunque sean los más interesantes), sino aprender bien la función $f$ que ya hemos descrito anteriormente.

\pagebreak
\subsection{Preprocesado de los datos}

\subsubsection{Eliminación de outliers}

Antes de realizar normalización, debemos eliminar los \emph{outliers}. Estos son aquellos valores que están a una distancia de la media de más de 3 veces la desviación típica. Tenemos distintas características por cada dato, así que definimos los \emph{outliers} como aquellas filas que, en alguna de las variables que definen las columnas, se desvían como ya hemos especificado. Podríamos haber optado por técnicas que detectasen \emph{outliers} basándonos en más de una variable, pero por simplicidad, seguimos el procedimiento ya indicado. La opción multivariable preserva más datos, pero tenemos un \emph{dataset} lo suficientemente grande como para permitir el borrar más filas.

En nuestro caso, por ser menos restrictivos, establecemos el límite en 4 veces la desviación típica.

Todo esto está fundamentado en que, en una distribución normal, el $99.74\%$ de los datos se encuentran en el intervalo $[\mu - \sigma, \mu + \sigma]$. Al tener una gran cantidad de datos, por el teorema central del límite, podemos suponer que nuestra distribución de datos se aproxima a una distribución normal (multivariante al tener varias variables).

Además, hacemos esto antes de estandarizar, pues para estandarizar, usamos los estadísticos media y desviación típica, que son muy sensibles a los valores \emph{outliers} \cite{scikit_scale_with_outliers:online}. También afectan los \emph{outliers} al procedimiento de \emph{PCA}, pues se basa en estadísticos altamente afectados por dichos \emph{outliers} \cite{pca_medium:online}.

Previamente hemos justificado que no vamos a borrar \emph{outliers} respecto a la variable de salida.

El código que borra los outliers se encuentra en la función \lstinline{remove_outliers} \footnotemark. Usamos una orden de acceso de la librería \lstinline{pandas} en la que usamos el estadístico \lstinline{zscore}, de la librería \lstinline{scipy}. Este valor \lstinline{zscore} lo que mide es el número de desviaciones típicas en las que un punto dista de la media de la variable aleatoria, es decir:

\footnotetext{En la sección \emph{\ref{consideraciones}. \nameref{consideraciones}} indicamos la librería adicional que hemos empleado para el cálculo del \emph{z-score}}

$$z_{score}(x) := \frac{x - \mu}{\sigma}$$

Borramos aquellos valores que, en alguna variable aleatoria columna, tengan un valor absoluto de \lstinline{zscore} mayor o igual que 4.

Tras ejecutar el eliminado de \emph{outliers}, eliminamos el 8.78\% de los datos. Teniendo en cuenta la gran cantidad de datos de los que disponemos, junto al hecho de que en \cite{original_paper_reg:paper} comentan los autores que se quedan con el 67\% de los datos originales, para acabar con un \emph{dataset} de calidad, queda justificada esta pérdida de datos en pro de un conjunto de datos más limpio y de calidad.

En este punto nos preocupamos de haber eliminado, sin fijarnos en la variable de salida, filas con valores de salida interesantes o que desbalanceen aún más el conjunto de datos. Sin embargo, computamos unas cuantas estadísticas de las filas eliminadas, respecto de la columna de salida $T_c$. La media de los datos eliminados es 12.82, y la desviación típica es 22.34. Por tanto estamos eliminando mayoritariamente filas con $T_c$ bajos, así que no vemos que estemos introduciendo aún más desbalanceo.

\subsubsection{Estandarización}

En este proceso, buscamos que las variables aleatorias de nuestro conjunto de datos queden con media cero y desviación típica uno. Este proceso no hace que las variables estén en un rango de valores similares, sin embargo, en este problema de regresión no parece ser importante. No usamos técnicas basadas en proximidad como \emph{nearest neighbour} o \emph{SVM}, en las que una normalización sería más adecuada la normalización \cite{normalization_vs_standarization:online}. En normalización conseguiríamos que todas las variables estuviesen en el rango $[0, 1]$.

En estandarización, aplicamos la siguiente operación a todas las variables aleatorias que conforman nuestro conjunto de datos:

$\mathbb{X'} = \frac{\mathbb{X} - \mu}{\sigma}$

El código usado para estandarizar se encuentra en \lstinline{standarize_dataset}. De nuevo, recibe como parámetro el conjunto de test. Este conjunto no se usa para calcular la transformación que representa la estandarización. Este cálculo solo toma información de los datos de entrenamiento. Por tanto, sobre el conjunto de test, solo se aplica la misma transformación.

En dicho código usamos la clase \lstinline{StandarScaler} de \lstinline{sklearn}, que hace justo lo que hemos especificado \cite{sklearn_std_scaler:online}.

Aplicamos estandarización tanto al conjunto original de datos, al que solo hemos borrado outliers, y tras aplicar la técnica \emph{PCA}, aplicaremos de nuevo estandarización a los datos transformados. Notar que es importante la estadarización previamente a \emph{PCA}, pues como se comentará más tarde, rangos de la desviación típica dispares puede hacer que una variable domine sobre las demás desproporcionadamente, obteniendo resultados no deseados.

En dicho código se puede observar que no estamos estandarizando la variable de salida, pues esto no tiene sentido. En datos no vistos en el entrenamiento, nos llegan variables de entrada pero no de salida. Así que la predicción debe hacerse en el conjunto aleatorio que representa la variable aleatoria de salida original.

Mostramos algunos de los datos, no todos pues no tiene mayor interés, del conjunto de datos original tras estandarizar:

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
\textbf{name} &                                      \textbf{mean} &    \textbf{median} &       \textbf{var} &       \textbf{sdt} &       \textbf{min} &       \textbf{max} &       \textbf{p25} &       \textbf{p75} \\
\hline
numberof\_elements              &  1.95e-16& -0.07&  1.00&  1.00& -2.15&  3.38& -0.77&  0.61 \\
meanatomic\_mass                & -5.66e-17& -0.08&  1.00&  1.00& -2.71&  4.09& -0.50&  0.43 \\
wtdmean\_atomic\_mass            &  3.29e-17& -0.36&  1.00&  1.00& -1.98&  4.05& -0.62&  0.39 \\
gmeanatomic\_mass               & -2.81e-16& -0.15&  1.00&  1.00& -2.12&  4.44& -0.43&  0.22 \\
\ldots &  \ldots & \ldots & \ldots & \ldots &  \ldots & \ldots & \ldots & \ldots \\
wtdrange\_Valence               & -4.75e-17& -0.42&  1.00&  1.00& -1.50&  5.59& -0.57&  0.44 \\
stdValence                     & -3.22e-16& -0.08&  1.00&  1.00& -1.72&  4.42& -0.76&  0.75 \\
wtdstd\_Valence                 &  7.12e-17& -0.38&  1.00&  1.00& -1.47&  5.08& -0.80&  0.75 \\
\hline
    \end{tabular}
    \caption{Conjunto de datos sin aplicar \emph{PCA} tras la \emph{estandarización}}
    \label{tabla_sin_pca}
\end{table}

En la tablas vemos que acabamos con desviación típica 1 y media prácticamente cero (notar que tenemos valores en órdenes de magnitud de $10^{-16}$ o $10^{-17}$, es decir, prácticamente cero). Aunque esta técnica de escalado no se centre en normalizar el rango de valores, si que hace que los rangos se normalicen algo. Por ejemplo, en la \emph{Tabla \ref{tabla_sin_pca}. \nameref{tabla_sin_pca}}, el valor de \emph{meanatomic\_mass} se mueve ahora en un rango $[-2.71, 4.09]$, mientras que en la \emph{Tabla \ref{Tabla con los estadísticos de las features}. \nameref{Tabla con los estadísticos de las features}} podemos ver que se movía en un rango $[6.94, 208.98]$. Así que aunque no estemos explícitamente preocupándonos por el rango de las variables, sí que estamos haciendo que no sean rangos tan amplios ni rangos tan dispares entre distintas variables.

\subsubsection{Principal Component Analysis}

Con esta técnica buscamos reducir la dimensionalidad de nuestro conjunto de datos, manteniendo el máximo de la variabilidad original (que es lo que nos permite llevar a cabo un proceso de aprendizaje).

Buscamos una base ortonormal de un espacio $\mathbb{R}^{\hat{d}}$ donde $\hat{d} << d$, es decir, el nuevo espacio euclídeo tiene una dimensión mucho menor que el espacio original. Todo esto gracias a calcular los valores propios de la matriz de covarianzas del conjunto de datos original \cite{pca_wikipedia:online} \cite{pca_article:online}. Con ello, y usando propiedades de vectores y espacios propios, expresamos en espacio con vectores que están linealmente incorrelados.

Además, esta técnica devuelve los vectores de la base ordenados según la varianza que explican del conjunto de datos original.

En nuestro caso, esta transformación del espacio la realizamos gracias a la función \lstinline{apply_PCA}. Podemos especificar el número de variables con el que nos queremos quedar, como el porcentaje de varianza que queremos alcanzar. Notar que pasamos como parámetro el conjunto de test. Este conjunto de test \textbf{no se usa para calcular la transformación}, solo se pasa para aplicar la misma transformación calculada, de nuevo, exclusivamente usando los datos del conjunto de entrenamiento.

Un detalle destacable es que antes de aplicar \emph{PCA} debemos estandarizar el conjunto de datos, pues de otra forma trabajaríamos con peores resultados. De otra forma, rangos dispares en las desviaciones permitiría que una variable dominase demasiado a las otras variables al tener las estadísticas en rangos dispares \cite{normalize_before_pca:online}.

Cuando buscamos un 99\% de varianza explicada, obtenemos una transformación del espacio $\mathbb{R}^{81}$ al espacio $\mathbb{R}^{30}$, es decir, hemos acabado con el $37.04\%$ de las variables originales, manteniendo el $99\%$ de la variableidad del dataset original. Con esto seremos capaces de realizar transformaciones no lineales de los datos, buscando un mejor ajuste lineal a la muestra de entrenamiento. Una reducción tan drástica tiene sentido, pues tenemos 8 variables dependientes de los átomos, que ya de por sí estarán en cierto grado correladas, y 10 estadísticas moleculares, que podemos suponer altamente correladas.


Las estadísticas del conjunto de datos tras la transformación se refleja en la siguiente tabla:

\begin{table}[H]
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
    \textbf{Columna}                             &    \textbf{mean}&       \textbf{median}&           \textbf{var}&          \textbf{sdt}&         \textbf{min}&           \textbf{max}&          \textbf{p25}&          \textbf{p75}  \\
0    & -1.35e-16&  -3.05&   31.45&   5.60&   -8.69&   18.00&  -4.69&   4.44 \\
1    &  1.23e-17&  -0.29&    8.54&   2.92&   -7.42&   17.15&  -1.69&   1.10 \\
2    &  1.53e-16&   0.26&    7.66&   2.76&  -10.17&   10.29&  -1.78&   2.17 \\
3    &  1.15e-17&   0.09&    6.42&   2.53&   -8.03&   11.75&  -1.48&   1.36 \\
4    &  4.50e-17&   0.23&    4.79&   2.18&   -8.72&   11.06&  -1.00&   1.01 \\
5    &  3.43e-17&  -0.28&    3.05&   1.74&   -7.73&   10.46&  -1.15&   0.91 \\
6    &  3.60e-18&  -0.11&    2.95&   1.71&   -7.06&    9.57&  -0.82&   0.78 \\
7    & -5.65e-17&  -0.05&    2.56&   1.60&   -7.79&    9.03&  -0.65&   0.78 \\
8    & -8.47e-18&  -0.10&    1.90&   1.38&   -6.53&    8.11&  -0.69&   0.73 \\
9    & -7.70e-18&  -0.08&    1.60&   1.26&   -6.10&    8.13&  -0.81&   0.76 \\
10   &  8.10e-19&   0.04&    1.46&   1.21&   -6.82&    7.16&  -0.57&   0.64 \\
11   & -2.40e-17&  -0.04&    1.17&   1.08&   -3.88&    6.32&  -0.46&   0.50 \\
12   &  2.50e-17&   0.04&    0.94&   0.97&   -4.42&    4.84&  -0.60&   0.52 \\
13   &  8.87e-18&  -0.05&    0.81&   0.90&   -4.47&    4.75&  -0.42&   0.48 \\
14   &  3.25e-17&  -0.02&    0.78&   0.88&   -4.08&    6.73&  -0.36&   0.38 \\
15   & -5.81e-17&  -0.02&    0.63&   0.79&   -4.76&    4.84&  -0.40&   0.42 \\
16   & -2.21e-17&  -0.09&    0.58&   0.76&   -3.44&    5.50&  -0.39&   0.27 \\
17   &  1.80e-17&  -0.01&    0.44&   0.66&   -2.50&    4.05&  -0.37&   0.38 \\
18   &  1.38e-17&  -0.01&    0.39&   0.62&   -3.76&    4.72&  -0.31&   0.27 \\
19   & -1.69e-17&  -0.03&    0.30&   0.55&   -2.42&    3.17&  -0.32&   0.29 \\
20   &  1.81e-17&  -0.03&    0.24&   0.49&   -1.75&    2.88&  -0.34&   0.29 \\
21   & -2.99e-17&  -0.00&    0.22&   0.47&   -2.56&    3.53&  -0.21&   0.21 \\
22   &  1.13e-17&  -0.00&    0.20&   0.45&   -2.59&    4.32&  -0.26&   0.28 \\
23   &  4.11e-18&  -0.00&    0.16&   0.40&   -2.22&    3.08&  -0.22&   0.24 \\
24   & -3.87e-18&  -0.00&    0.15&   0.39&   -2.20&    2.94&  -0.23&   0.22 \\
25   & -4.30e-17&  -0.00&    0.14&   0.38&   -2.09&    2.79&  -0.21&   0.21 \\
26   & -2.97e-17&  -0.02&    0.13&   0.37&   -2.01&    2.37&  -0.23&   0.21 \\
27   & -2.85e-18&   0.00&    0.11&   0.34&   -1.67&    1.92&  -0.17&   0.17 \\
28   & -1.64e-17&   0.00&    0.10&   0.32&   -2.03&    2.64&  -0.16&   0.15 \\
29   &  3.84e-18&   0.01&    0.09&   0.30&   -1.60&    2.05&  -0.17&   0.16 \\
30   & -3.34e-17&  -0.02&    0.08&   0.28&   -1.47&    1.92&  -0.15&   0.15 \\

\hline

\hline
    \end{tabular}
    }
    \caption{Estadísticas de las \emph{features} tras aplicar \emph{PCA}}
\end{table}

Es claro que no controlamos la transformación, por tanto no tenemos interpretación de lo que representa cada columna. También es claro que tenemos desviaciones típicas en órdenes de magnitud distintas y rangos (mínimo y máximo) completamente dispares. Por tanto, tanto como con el conjunto de datos original como en el conjunto tras aplicar \emph{PCA}, es necesario realizar una estandarización o normalización de los datos.

Estandarizamos estos nuevos datos, pues es claro que los rangos y las desviaciones son bastante dispares. Mostramos algunos de los datos, no tiene mayor interés mostrar las 30 filas:

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
\textbf{name}&                \textbf{mean}&     \textbf{median}&          \textbf{var}&        \textbf{sdt}&       \textbf{min}&         \textbf{max}&       \textbf{p25}&        \textbf{p75} \\
\hline
0               &-1.16e-17  &-0.54     &1.00   &1.00 &-1.55    &3.21 &-0.83   &0.79 \\
1               & 1.87e-18  &-0.10     &1.00   &1.00 &-2.54    &5.87 &-0.57   &0.37 \\
2               &-1.42e-17  & 0.09     &1.00   &1.00 &-3.67    &3.71 &-0.64   &0.78 \\
\ldots &  \ldots & \ldots & \ldots & \ldots &  \ldots & \ldots & \ldots & \ldots \\
28              &-1.48e-17  & 0.01     &1.00   &1.00 &-6.33    &8.21 &-0.50   &0.46 \\
29              & 1.73e-17  & 0.05     &1.00   &1.00 &-5.23    &6.70 &-0.57   &0.53 \\
30              & 1.00e-17  &-0.06     &1.00   &1.00 &-5.13    &6.69 &-0.53   &0.54 \\
\hline
    \end{tabular}
    \caption{Conjunto de datos tras aplicar \emph{PCA} y \emph{estandarización}}
\end{table}

Y de nuevo, vemos que tenemos media aproximadamente cero y desviación típica igual a uno, como buscábamos

\pagebreak
\subsection{Selección del modelo}

En esta sección vamos a emplear la técnicas de \emph{Cross Validation} para la selección del modelo y los parámetros empleados durante el aprendizaje. Vamos a emplear dos veces \emph{CV} como vamos a detallar más adelante.

En realidad estamos usando la técnica de \emph{K-Fold Cross Validation}, con $K = 10$. En esta técnica, tomamos nuestro \emph{dataset} de entrenamiento y lo dividimos en $K$ \emph{folds} o subgrupos. Una vez hecho esto, realizamos $K$ veces el proceso de tomar un \emph{fold} como conjunto de validación, y los restantes $K-1$ \emph{folds} para entrenar el modelo candidato. Sobre el \emph{fold} de validación calculamos una métrica de error. Así tenemos $K$ entrenamientos con $K$ métricas, de las que podemos calcular estadísticas como media, mínimo y máximo, \ldots

En las funciones \lstinline{show_cross_validation_step1} y \lstinline{show_cross_validation_step2} aplicamos dos veces el proceso de \emph{Cross Validation}.

\subsubsection{Selección de la métrica de error}

En ambas fases de \emph{Cross Validation}, usaremos la misma métrica de error. En este caso, usamos el \textbf{Error cuadrático Medio}. Es una de las pocas métricas de error que conocemos para los problemas de regresión. De todas formas, elegimos esta métrica porque es fácilmente interpretable, el término cuadrático castiga más los valores que se predicen peor que otra métrica como el error absoluto medio. El \emph{ECM} es una métrica de error que se usa en el proceso de aprendizaje, directamente en el método de los mínimos cuadrados ordinario ordinario, o como término del error aumentado, donde el \emph{ECM} es el sumando de la métrica del error que va acompañada de un término de penalización de la parte de regularización.

A la hora de especificar la métrica de error con \lstinline{sklearn}, usamos el error cuadrático medio negativo. Esto porque \lstinline{sklearn} busca maximizar esta \emph{score} en otras funciones. Así, un problema de minimizar el error, pasa a ser un problema de maximización cambiando el signo de la función a optimizar \cite{sklearn_neg_err:online}.

\subsubsection{Primera etapa - Modelos candidatos}

A la hora de resolver este problema tenemos como modelo el ajuste de un hiperplano a los datos. Es decir, nuestra clase de funciones que representa el modelo viene dada por:

$$\mathcal{H} := \{f_w / w \in \mathbb{R}^D\}$$

donde $w \in \mathbb{R}^d$ son los parámetros que definen cada una de las hipótesis pertenecientes a la clase de funciones, que especifican la función de la siguiente forma:

$$f_w(x):= w^T x, \forall x \in \mathbb{R}^D$$

Así que lo buscamos elegir en esta fase de \emph{Cross Validation} es:

\begin{itemize}
    \item El conjunto de datos y la transformación que queremos aplicar sobre los datos: datos a los que no aplicamos PCA y sin transformaciones, datos a los que aplicamos PCA y aplicamos transformaciones polinómicas
    \item El algoritmo de aprendizaje: mínimos cuadrados ordinarios, Ridge o Lasso
\end{itemize}

Respecto a las transformaciones de los datos, a falta de conocimiento experto sobre el problema para guiar las transformaciones empleadas, probamos con transformaciones polinómicas $\phi_q$. Fijado el orden $q$, calculamos todos los polinomios en varias variables de hasta orden $q$. Por ejemplo, con $q=3$, podemos encontrar en el vector transformado los elementos $x, x^2, xy, x^2y, y^2x, xyz, \ldots$. En código esto lo conseguimos con la función de \lstinline{sklearn} \lstinline{PolynomialFeatures} \cite{sklearn_polynomial:online}.

El primer modelo consiste en minimizar el error cuadrático medio a través de la matriz pseudoinversa \cite{sklearn_linear_reg:online}. Por tanto, no tenemos que preocuparnos de parámetros de procesos iterativos como la tolerancia.

En segundo modelo consiste en minimizar el error aumentado en el que el término de penalización de regularización viene dado por $\lambda ||w||^2_2$. Con esto, favorecemos soluciones con valores en los pesos no muy grandes, aunque no necesariamente cero. Se realiza un proceso iterativo, donde los valores por defecto son \cite{sklearn_ridge:online}:

\begin{itemize}
    \item Máximo de iteraciones: 1000 iteraciones por defecto. A vista de nuestros datos, parece un número suficiente de iteraciones
    \item Tolerancia: por defecto, $10^{-3}$, diferencia de error mínima entre dos iteraciones. Teniendo en cuenta de que llegaremos a errores en el intervalo $[300, 1800]$, parece una tolerancia mucho más que aceptable
    \item Alpha: valor que nosotros hemos llamado en el curso $\lambda$. Término de penalización para la regularización. Lo establecemos nosotros a un valor de 0.05. Parece sensato pues en otros muchos problemas hemos visto que se emplea valores en el intervalo $[10^{-2}, 1]$. Además tenemos una dimensionalidad pequeña así que no parece necesario tomar un valor alto para $\lambda$
\end{itemize}

En el caso de Lasso, tenemos la misma situación que con Ridge pero tomando el error para la parte de regularización como $\sum_{k = 1}^{d} |w_k|$. En este caso, estamos favoreciendo el que muchos parámetros sean cero. Por tanto, es un buen modelo para selección de características. Usamos los mismos parámetros que especificados para Ridge.

Respecto al paso al código de los modelos ya detallados, podemos encontrar toda la información de \lstinline{sklearn} en la documentación oficial \cite{sklearn_linear_models_list:online}.

Estamos considerando dos regularizadores distintos (los que se usan en Lasso y Ridge) en vez de decantarnos directamente por uno de los dos en base a alguna intuición que tengamos sobre los datos. Si tuviésemos que elegir solo uno, elegiríamos \emph{Ridge} porque no estamos haciendo selección de características ni con una gran cantidad de variables. Sin embargo, estamos haciendo las transformaciones polinómicas sin tener mucha idea de si muchos de los monomios obtenidos van a ser útiles o no. Por eso confiamos en que Lasso no de peso a muchos monomios irrelevantes, dándole peso a algunos monomios relevantes de forma automática. Sin embargo, si esto falla, confiamos más en el empleo de \emph{Ridge}.

Por otro lado, como hemos sido capaces de emplear la pseudoinversa sin problemas de tiempos de cómputo, empleamos esta técnica \emph{one step solution} en vez de un enfoque iterativo que podríamos haber empleado con \emph{SGDRegressor}. Los momentos en los que el ordenador no es capaz de realizar los cómputos, es porque la transformación polinómica es demasiado grande.

\subsubsection{Resultados de \emph{Cross-Validation}, primera etapa}

Con la orden \lstinline{cv = KFold(n_splits=10, shuffle=True)} especificamos los 10 \emph{folds} y que se use mezclado aleatorio para elegir los elementos de los \emph{folds}.

En el conjunto de datos al que aplicamos \emph{PCA}, probamos las transformaciones polinómicas en el conjunto $\{1, 2\}$. No aplicamos transformaciones de orden superior pues nuestro ordenador no es capaz de calcular estas transformaciones. Al conjunto de datos al que no aplicamos \emph{PCA}, no aplicamos transformaciones polinómicas, pues de nuevo nuestro ordenador no es capaz realizar los cómputos

Al usar \emph{Lasso}, el código lanza algunos errores porque no alcanza solución estable en el número máximo de iteraciones dado. Sin embargo, el código automáticamente vuelve a intentarlo con otra solución inicial aleatoria, llegando a encontrar la solución en el número máximo de iteraciones especificado.

Los resultados de \emph{Cross Validation} se resumen en la siguiente tabla:

\begin{table}[H]
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|}
    \hline
    \textbf{Modelo} & \textbf{PCA \/ No PCA}& \textbf{Orden de la transformación polinómica} & \textbf{Valor medio} & \textbf{Valor mínimo} & \textbf{Valor máximo} \\
    \hline
    Lineal & PCA & 1 & -365.39 &-411.91 & -347.46 \\
    Lineal & PCA & 2 & -229.56 &-254.66 & -210.64 \\
    Ridge  & PCA & 1 & -365.59 &-397.05 & -345.15 \\
    Ridge  & PCA & 2 & -231.89 &-259.39 & -200.73 \\
    Lasso  & PCA & 1 & -365.45 &-402.61 & -343.86 \\
    Lasso  & PCA  & 2 & -231.36 &-267.28 & -213.01 \\
    Lineal & No PCA & 1 & -310.50 &-322.55 & -282.40 \\
    Ridge  & No PCA & 1 & -310.77 &-335.69 & -292.88 \\
    Lasso  & No PCA & 1 & -328.42 &-355.69 & -310.94 \\
    \hline
\end{tabular}}
    \caption{Resultados de \emph{Cross Validation}, primera fase}
\end{table}

La tabla deja claro que lo mejor es aplicar \emph{PCA} y transformaciones polinómicas de segundo grado. Sería interesante conocer los resultados de aplicar transformaciones de hasta orden 3 en una máquina más potente, con mas memoria para almacenar los cálculos. El claro ganador es tomar \emph{PCA}, transformación de orden 2 y modelo lineal normal. Sin embargo, el segundo mejor, \emph{PCA} con orden 2 y Lasso, difiere en error cuadrático medio por tan solo un valor de $1.8$. Como todavía no hemos encontrado un valor de $\lambda$ óptimo, elegimos este modelo a pesar de que tengamos un error ligeramente mayor, con la esperanza de que en la segunda fase de \emph{Cross Validation} consigamos un error menor.

En el \emph{dataset} al que no aplicamos PCA, es previsible que \emph{Lasso} iba a funcionar mal. Durante el curso hemos visto la regla práctica de que, para no tener problemas de generalización, es deseable que $N > 10 d_{VC}
$. Tenemos, tras toda la limpieza de los datos, algo más de 15.500 columnas, y por tanto, al trabajar con modelos lineales, podemos trabajar con un número de variables en el orden de $1500$ columnas. Es claro que con 81 columnas no nos interesa hacer que algunas columnas sean cero, que es lo que consigue \emph{Lasso}.

\subsubsection{\emph{Cross-Validation, segunda etapa}}

Como ya hemos justificado, elegimos regresión \emph{Lasso} sobre el conjunto de datos al que aplicamos \emph{PCA}.
En esta fase escogemos el valor de $\lambda$: parámetro de penalización de la regularización. El rango de valores escogido es $\lambda \in [10^{-3}, 1]$, basándonos en que el el primer \emph{step} hemos tomado $\lambda = 0.05$ y que estamos trabajando con una dimensionalidad alta, por lo tanto, queremos que la penalización no sea demasiado baja.

Los resultados de esta segunda fase se resumen en la siguiente fase:

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{Lambda} & \textbf{Valor medio} & \textbf{Valor mínimo} & \textbf{Valor máximo} \\
        \hline
        0.001 & -228.24 & -250.70 & -210.52 \\
        0.01  & -228.89 & -254.37 & -211.02 \\
        0.1   & -236.02 & -261.20 & -212.81 \\
        1     & -319.58 & -330.61 & -308.83 \\
        \hline
    \end{tabular}
    \caption{Resultados de \emph{Cross Validation}, segunda fase}
\end{table}

Con los resultados de esta tabla, elegimos el parámetro del regularizador como $\lambda = 10^{-3}$, con lo que ya estamos listos para entrenar sobre todo el conjunto de datos, elegidos todos los parámetros de nuestro modelo final.

\pagebreak
\subsection{Entrenamiento sobre todo el \emph{train\_dataset} para seleccionar el modelo final}

En este punto está justificado el que usemos el conjunto original de datos, al que hemos eliminado \emph{outliers} y estandarizado. Tras esto, aplicamos \emph{PCA} obteniendo 30 variables, y estandarizando de nuevo.. Además, usaremos como parámetros:

\begin{itemize}
    \item $\lambda = 10^{-3}$
    \item Máximo de iteraciones: 1000 iteraciones
    \item Tolerancia: $10^{-3}$
\end{itemize}

Con esto obtenemos los siguientes resultados:

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Conjunto de datos} & \textbf{Error cuadrático medio} & \textbf{Error absoluto medio} & \textbf{$R^2$} \\
    \hline
    Entrenamiento & 216.97 & 10.42 & 0.81 \\
    Test & 223.29 & 10.70 & 0.80 \\
    \hline
\end{tabular}
    \caption{Resultados del entrenamiento}
\end{table}

\subsubsection{Análisis de los resultados}

El error cuadrático medio viene dado por $\frac{1}{N} \sum^N |g(x) - y|^2$ donde $g$ es la función aprendida, $x$ el dato de entrada e $y$ la etiqueta verdadera asociada. El error absoluto medio viene dado por $\frac{1}{N} \sum^N |g(x) - y|$, que es algo más interpretable que el error cuadrático medio. $R^2$ es el coeficiente de determinación lineal.

Es claro que no hemos tenido problemas de \emph{overfitting}, pues el error en test no es demasiado dispar al error en la muestra. Los coeficientes de correlación son bastante buenos, estamos explicando el $80\%$ de la varianza de la muestra de test con nuestro modelo lineal.

El error absoluto medio en el test se puede interpretar como que de media, para cada dato estamos prediciendo con un error $\pm 10.7 K$. Respecto a la bondad de nuestros resultados, en el paper original \cite{original_paper_reg:paper} se comenta que obtienen resultados con error \emph{rmse} de $9.5K$, donde \emph{rmse} es la raíz del error cuadrático medio. Nosotros obtenemos un \emph{rmse} en test de $14.94K$. Por tanto, con un modelo mucho más simple que el empleado en \cite{original_paper_reg:paper}, basado en árboles, hemos obtenido un \emph{rmse} cercano a este valor de referencia de $9.5K$.

Además, la incertidumbre $\pm 10.7 K$ en datos que de media tiene un $T_c = 34.37$ y que se mueve en un rango $T_c \in [0.00, 185.00]$ parece aceptable, aunque claramente es mejorable con el empleo de modelos más complejos, como demuestran los resultados obtenidos en \cite{original_paper_reg:paper}.

Usaremos un \emph{baseline} para ver si hemos aprendido de forma efectiva una función sobre los datos que no sea trivial. Consideramos la función \emph{baseline} que a todo $x$ le hace corresponder el valor medio de la variable $T_c$. Esto lo conseguimos con el \lstinline{DummyRegressor} de la librería \lstinline{sklearn}. Los resultados obtenidos son:

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Conjunto de datos} & \textbf{Error cuadrático medio} & \textbf{Error absoluto medio} & \textbf{$R^2$} \\
    \hline
    Entrenamiento & 1173.58 & 29.34 &  0.0 \\
    Test & 1172.19 & 29.23 & -3.70e-5 \\
    \hline
\end{tabular}
    \caption{Resultados del baseline usando la media}
\end{table}

Es claro que estamos obteniendo resultados notablemente mejores, y por tanto, podemos concluir que hemos conseguido aprender de forma no trivial a partir de los datos de la muestra.

Por tanto, la principal mejora al trabajo presentado sería considerar modelos más avanzados y adecuados al tipo de problema presentado. De todas formas, queda probada la potencia de los modelos lineales, a pesar de su simplicidad, cuando se emplean el preprocesado y la transformación adecuado del conjunto de datos.

\pagebreak
\section{Problema de clasificación}

\subsection{Exploración de los datos}

==> Hay 11 clases
==> Clases balanceadas como mostramos en la grafica de barras
==> Mostrar la tabla con todas las variables
    ==> Muchas variables contiguas que tienen rangos muy similares, lo que nos hacen sospechar

\subsubsection{Eliminado de outliers}

=> Quitamos el 6\% de los datos

\subsubsection{Cross Validation}


=> Logistic Regression le cuesta mucho calcular
=> SVC and NuSVC implement the “one-versus-one” approach for multi-class classification => https://scikit-learn.org/stable/modules/svm.html


% Model LogReg, pol_order: 1, C: 0.9:
%         Media: 0.9183421869511518
%         Minimo: 0.9150102810143934
%         Maximo: 0.9239204934886909
% Model LogReg, pol_order: 1, C: 0.95:
%         Media: 0.9187761071564464
%         Minimo: 0.9120201096892139
%         Maximo: 0.9250628284212932
% Model LogReg, pol_order: 1, C: 1.0:
%         Media: 0.9187076088277639
%         Minimo: 0.9111263422435458
%         Maximo: 0.9220927575965273
% Model LogReg, pol_order: 1, C: 1.05:
%         Media: 0.9190506382248239
%         Minimo: 0.912268677176148
%         Maximo: 0.9239031078610603
% Model LogReg, pol_order: 1, C: 1.1:
%         Media: 0.9187760653891728
%         Minimo: 0.9131627056672761
%         Maximo: 0.9232350925291295
% Model LogReg, pol_order: 1, C: 1.15:
%         Media: 0.9190731873316519
%         Minimo: 0.9136394790952708
%         Maximo: 0.9259766963673749
% Model LogReg, pol_order: 1, C: 1.2:
%         Media: 0.9187763786437246
%         Minimo: 0.9145533470413525
%         Maximo: 0.926416819012797
% Model LogReg, pol_order: 2, C: 0.9:
%         Media: 0.9649286677618045
%         Minimo: 0.9611606122915238
%         Maximo: 0.9702992917523418

\pagebreak

\section{Consideraciones adicionales} \label{consideraciones}

Estamos usando la librería \lstinline{scipy} para el borrado de los outliers. En concreto, para hacer una \emph{query} a \lstinline{pandas} en la que calculamos el \emph{z-score}. Esto se ve en la función \lstinline{remove_outliers}


\pagebreak
% Bibliografia
\bibliography{./References}
\bibliographystyle{ieeetr}

\end{document}
