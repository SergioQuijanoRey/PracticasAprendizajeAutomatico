\documentclass[11pt]{article}

% Paquetes
%===================================================================================================

% Establecemos los márgenes
\usepackage[a4paper, margin=1in]{geometry}

% Separacion entre parrafos
\setlength{\parskip}{1em}

% Paquete para incluir codigo
\usepackage{listings}

% Paquete para incluir imagenes
\usepackage{graphicx}
\graphicspath{ {./images/} }

% Para fijar las imagenes en la posicion deseada
\usepackage{float}

% Para que el codigo acepte caracteres en utf8
\lstset{literate=
  {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
  {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
  {à}{{\`a}}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
  {À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
  {ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
  {Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
  {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
  {Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
  {ã}{{\~a}}1 {ẽ}{{\~e}}1 {ĩ}{{\~i}}1 {õ}{{\~o}}1 {ũ}{{\~u}}1
  {Ã}{{\~A}}1 {Ẽ}{{\~E}}1 {Ĩ}{{\~I}}1 {Õ}{{\~O}}1 {Ũ}{{\~U}}1
  {œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
  {ű}{{\H{u}}}1 {Ű}{{\H{U}}}1 {ő}{{\H{o}}}1 {Ő}{{\H{O}}}1
  {ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
  {€}{{\euro}}1 {£}{{\pounds}}1 {«}{{\guillemotleft}}1
  {»}{{\guillemotright}}1 {ñ}{{\~n}}1 {Ñ}{{\~N}}1 {¿}{{?`}}1 {¡}{{!`}}1
}

% Para que no se salgan las lineas de codigo
\lstset{breaklines=true}

% Para que los metadatos que escribe latex esten en español
\usepackage[spanish]{babel}

% Para la bibliografia
% Sin esto, los enlaces de la bibliografia dan un error de compilacion
\usepackage{url}

% Para mostrar graficas de dos imagenes, cada una con su caption, y con un caption comun
\usepackage{subcaption}

% Simbolo de los numeros reales
\usepackage{amssymb}

% Para que los codigos tengan una fuente distinta
\usepackage{courier}

\lstdefinestyle{CustomStyle}{
  language=Python,
  numbers=left,
  stepnumber=1,
  numbersep=10pt,
  tabsize=4,
  showspaces=false,
  showstringspaces=false
  basicstyle=\tiny\ttfamily,
}

% Para incluir tablas en csv
\usepackage{csvsimple}

% Para referenciar secciones usando el nombre de las secciones
\usepackage{nameref}

% Para enumerados dentro de enumerados
\usepackage{enumitem}

% Metadatos del documento
%===================================================================================================
\title{
    {Aprendizaje Automático - Tercera Práctica}\\
    {Dos caso de uso reales}\\  % TODO -- repensar el titulo de la practica
    {Modelos Lineales}
}

\author{
    {Sergio Quijano Rey - 72103503k}\\
    {4º Doble Grado Ingeniería Informática y Matemáticas}\\
    {sergioquijano@correo.ugr.es}
}

\date{\today}

% Separacion entre parrafos
\setlength{\parskip}{1em}


% Contenido del documento
%===================================================================================================
\begin{document}

% Portada del documento
\maketitle
\pagebreak

% Indice de contenidos
\tableofcontents

% Lista de figuras
\listoffigures

% Bibliografia
% TODO -- ponerlo al final del archivo?
\bibliography{./References}
\bibliographystyle{ieeetr}

\pagebreak

\section{Problema de regresión}

Los superconductores tienen la interesante propiedad de poder lograr resistencias al paso de la corriente muy cercanas a $0\Omega$. Sin embargo, esto solo ocurre cuando están por debajo de la temperatura crítica para este fenómeno, denotada como $T_c$.

Un superconductor con un un valor de $T_c$ muy bajo no resultaría práctico en aplicaciones de ingeniería, pues para aprovechar sus propiedades interesantes debería realizarse un proceso de enfriamiento que potencialmente consumiría mucha energía. Por tanto, es interesante conocer los valores de $T_c$ de los superconductores, para determinar si es viable o no su aplicación en distintos problemas .

No existe ningún modelo teórico para predecir el valor de $T_c$ de nuevos superconductores, por tanto es interesante plantear un modelo de regresión de aprendizaje automático para predecir dicho valor de $T_c$.

\footnotetext{Del paper que se presenta en la página del \emph{dataset} de UCI, \cite{original_paper_reg:paper} }

\subsection{Exploración del problema}

\subsubsection{Descripción del problema}

Disponemos de dos archivos, \lstinline{train.csv} y \lstinline{unique_m.csv}. Este último archivo contiene las fórmulas químicas desglosadas de los superconductores con los que trabajamos. \lstinline{train.csv} contiene 81 características de los superconductores, y el valor de $T_c$ que queremos predecir.

En el propio paper que se encuentra en la página del dataset con el que trabajamos, de UCI, se adjunta el \emph{paper} publicado que motiva el uso del \emph{dataset}, en el que se explica el tratamiento de los datos. En dicho tratamiento, se extraen las ya comentadas 81 \emph{features} de alto nivel. Por tanto, no parece posible que seamos capaces de obtener, a través de conocimiento experto del problema, más \emph{features} útiles para resolver este problema.

Además, en el paper se detalla el proceso de extracción de las 81 \emph{features} a partir de las fórmulas químicas expuestas en \lstinline{unique_m.csv}. Por tanto, usaremos esta información de alto nivel y no atenderemos a la información que nos pueda proporcionar \lstinline{unique_m.csv}.

En el mismo paper, el autor comenta que \emph{"We take an entirely data-driven approach"}, por lo tanto, esto junto a nuestra falta de conocimiento sobre el problema, justifica que usemos ténicas estadísticas para establecer el conjunto de características a emplear, y una ténica como \emph{cross-validation} para seleccionar el modelo a emplear, las transformaciones sobre los datos y distintos parámetros referentes al modelo escogido.

\footnotetext{ \cite{original_paper_reg:paper} }

\subsubsection{Descripción de las caracteríticas}

De nuevo, en el paper original \cite{original_paper_reg:paper} se describe el proceso de extracción de características, que pasamos a resumir brevemente.

Se parte de las siguientes propiedades de los elementos que componen los distintos superconductores:

% TODO -- esta tabla esta muy manchada
\begin{table}
\begin{center}
\begin{tabular}{|c|c|}
    Variable & Descripción \\
    \hline
    Atomic Mass & Total proton and neutron rest masses \\
    First Ionization Energy & Energy required to remove a valence electron \\
    Atomic Radius & Calculated atomic radius \\
    Density & Density at standard temperature and pressure \\
    Electron Affinity & Energy required to add an electron to a neutral atom \\
    Fusion Heat & Energy to change from solid to liquid without temperature change \\
    Thermal Conductivity &  Thermal conductivity coefficient $\kappa$ \\
    Valence & Typical number of chemical bonds formed by the element \\
    \hline
\end{tabular}
\end{center}
    \caption{Propiedades de los elementos usadas para crear las \emph{features} }
\end{table}

Las \emph{features} más importantes a la hora de predecir $T_c$ son aquellas basadas en la \emph{thermal conductivity}, \emph{atomic radius}, \emph{valence}, \emph{electron affinity}, y \emph{atomic mass} \cite{original_paper_reg:paper}. Con esto podríamos pensar en descartar el resto de \emph{features} que no se basen en las anteriores, sin embargo, no tenemos el conocimiento suficiente sobre el problema para realizar este descarte con confianza, delegando esta decisión a la técnica \emph{Principal Componente Analysis} que más adelante desarrollaremos.

A partir de esto, se calculan las siguientes \emph{features}. Por cada material, en base a sus elementos, se calculan los siguientes estadísticos por cada \emph{feature} mostrada en la anterior tabla:

Algo importante a destacar es que la unidad de temperatura para $T_c$ es \emph{Kelvin}, por lo que esta variable estará acotada inferiormente por cero. La temperatura ambiente en kelvin está en torno a los $298K$, por lo tanto, valores cercanos a esta referencia serán los más interesantes a la hora de escoger o no un material como superconductor.

\begin{itemize}
    \item Mean
    \item Weighted Mean
    \item Geometric Mean
    \item Weighted Geometric Mean
    \item Entropy
    \item Weighted Entropy
    \item Range
    \item Weighted Range
    \item Standard Deviation
    \item Weighted Standard Deviation
\end{itemize}

Las fórmulas para cada estadístico se pueden consultar en el ya mencionado \emph{paper} \cite{original_paper_reg:paper}. Notar que tenemos siempre el estadístico y su versión ponderada.

Teniendo 8 variables, y 10 estadísticas por cada variable, llegamos a 80 características en el dataset. La característica que falta para llegar a las 81, es el número de elementos que compone la molécula del superconductor.

\subsubsection{Exploración del \emph{Dataset}}

Antes de empezar a explorar los datos del problema, separamos train, test => No queremos saber nada sobre el test que nos pueda influir => para evitar data snooping
Mostramos las estadisticas de las variables sin normalizar => tabla que he construido con pandas
Rangos de variables dispares => justifica la necesidad de normalizar los datos
Todos los datos son reales o enteros => no es necesario realizar otro tipo de codificaciones
Mostramos el boxplot de la variable de salida => Problema con la representación de la muestra. Muestra algo sesgada, pocos ejemplos en la muestra para valores de temperatura altos. Mostrar la figure 4 del paper. Pocos superconductores de alto Tc que son precisamente los que interesan

\subsection{Preprocesado de los datos}

\subsubsection{Eliminación de outliers}

Eliminamos outliers => valores que se desvían más de 4 veces la desviación típica
Eliminamos outliers antes de normalizar, porque estamos normalizando con la media y la desviacion tipica, los outliers pueden afectar a una buena normalizacion: https://scikit-learn.org/stable/modules/preprocessing.html\#scaling-data-with-outliers => esta en el .bib
Outliers con una sola variable por simplicidad => tenemos muchos datos, la pérdida no va a ser muy grande
Decir lo que es el Z-score (numero de desviaciones típicas en las que nos alejamos de la media). Quitamos filas con z-score mayor que 4
Si suponemos una distribucion normal => 99.74\% de los datos en [-3 std, 3 std]
Quitamos el 8.5\% de los datos. En el paper se cargan el 40\% de los datos, así que parece buena idea eliminar estos datos
Mostramos de nuevo la variable de salida => sigue funcionando bien porque está toda la variabilidad

\subsubsection{Normalización}

Ya hemos justificado la necesidad de aplicar normalizacion
Aplicamos normalización bien => sin tener en cuenta información del test => mostrar el código porque puede ser relevante
Comentar cómo estamos usando el scaler de sklearn



\pagebreak
\section{Problema de clasificación}

\end{document}
