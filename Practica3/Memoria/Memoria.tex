\documentclass[11pt]{article}

% Paquetes
%===================================================================================================

% Establecemos los márgenes
\usepackage[a4paper, margin=1in]{geometry}

% Separacion entre parrafos
\setlength{\parskip}{1em}

% Paquete para incluir codigo
\usepackage{listings}

% Paquete para incluir imagenes
\usepackage{graphicx}
\graphicspath{ {./images/} }

% Para fijar las imagenes en la posicion deseada
\usepackage{float}

% Para que el codigo acepte caracteres en utf8
\lstset{literate=
  {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
  {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
  {à}{{\`a}}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
  {À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
  {ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
  {Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
  {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
  {Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
  {ã}{{\~a}}1 {ẽ}{{\~e}}1 {ĩ}{{\~i}}1 {õ}{{\~o}}1 {ũ}{{\~u}}1
  {Ã}{{\~A}}1 {Ẽ}{{\~E}}1 {Ĩ}{{\~I}}1 {Õ}{{\~O}}1 {Ũ}{{\~U}}1
  {œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
  {ű}{{\H{u}}}1 {Ű}{{\H{U}}}1 {ő}{{\H{o}}}1 {Ő}{{\H{O}}}1
  {ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
  {€}{{\euro}}1 {£}{{\pounds}}1 {«}{{\guillemotleft}}1
  {»}{{\guillemotright}}1 {ñ}{{\~n}}1 {Ñ}{{\~N}}1 {¿}{{?`}}1 {¡}{{!`}}1
}

% Para que no se salgan las lineas de codigo
\lstset{breaklines=true}

% Para que los metadatos que escribe latex esten en español
\usepackage[spanish]{babel}

% Para la bibliografia
% Sin esto, los enlaces de la bibliografia dan un error de compilacion
\usepackage{url}

% Para mostrar graficas de dos imagenes, cada una con su caption, y con un caption comun
\usepackage{subcaption}

% Simbolo de los numeros reales
\usepackage{amssymb}

% Para que los codigos tengan una fuente distinta
\usepackage{courier}

\lstdefinestyle{CustomStyle}{
  language=Python,
  numbers=left,
  stepnumber=1,
  numbersep=10pt,
  tabsize=4,
  showspaces=false,
  showstringspaces=false
  basicstyle=\tiny\ttfamily,
}

% Para incluir tablas en csv
\usepackage{csvsimple}

% Para referenciar secciones usando el nombre de las secciones
\usepackage{nameref}

% Para enumerados dentro de enumerados
\usepackage{enumitem}

% Para mejores tablas
\usepackage{tabularx}

% Metadatos del documento
%===================================================================================================
\title{
    {Aprendizaje Automático - Tercera Práctica}\\
    {Dos caso de uso reales}\\  % TODO -- repensar el titulo de la practica
    {Modelos Lineales}
}

\author{
    {Sergio Quijano Rey - 72103503k}\\
    {4º Doble Grado Ingeniería Informática y Matemáticas}\\
    {sergioquijano@correo.ugr.es}
}

\date{\today}

% Separacion entre parrafos
\setlength{\parskip}{1em}

% Contenido del documento
%===================================================================================================
\begin{document}

% Portada del documento
\maketitle
\pagebreak

% Indice de contenidos
\tableofcontents

% Lista de figuras
\listoffigures

% Lista de tablas
\listoftables

\pagebreak

\section{Problema de regresión}

Los superconductores tienen la interesante propiedad de poder lograr resistencias al paso de la corriente muy cercanas a $0\Omega$. Sin embargo, esto solo ocurre cuando están por debajo de la temperatura crítica para este fenómeno, denotada como $T_c$.

Un superconductor con un un valor de $T_c$ muy bajo no resultaría práctico en aplicaciones de ingeniería, pues para aprovechar sus propiedades interesantes debería realizarse un proceso de enfriamiento que potencialmente consumiría mucha energía. Por tanto, es interesante conocer los valores de $T_c$ de los superconductores, para determinar si es viable o no su aplicación en distintos problemas.

No existe ningún modelo teórico para predecir el valor de $T_c$ de nuevos superconductores, por tanto es interesante plantear un modelo de regresión de aprendizaje automático para predecir dicho valor de $T_c$ \cite{original_paper_reg:paper}.

\subsection{Exploración del problema}

\subsubsection{Descripción del problema}

Disponemos de dos archivos, \lstinline{train.csv} y \lstinline{unique_m.csv}. Este último archivo contiene las fórmulas químicas desglosadas de los superconductores con los que trabajamos. \lstinline{train.csv} contiene 81 características de los superconductores, y el valor de $T_c$ que queremos predecir.

En el propio paper \cite{original_paper_reg:paper} que se encuentra en la página del dataset con el que trabajamos, de UCI, se explica el tratamiento de los datos. En dicha sección, se detalla el proceso de extracción de las 81 características. A partir de las fórmulas codificadas en \lstinline{unique_m.csv}, se extraen propiedades de los átomos que forman las moléculas. Al ser moléculas con más de un átomo, se toman estadísticos de las propiedades. Estas propiedades y estadísticos se detallan en \emph{\ref{descripcion_caracteristicas}. \nameref{descripcion_caracteristicas}}.

Por tanto, no parece factible que seamos capaces de obtener, a partir de conocimiento experto del problema, más \emph{features}, de más alto nivel a ser posible, que resulten útiles para resolver el problema. Consecuentemente, no usaremos la información que nos pueda proporcionar \lstinline{unique_m.csv}, ignorando este \emph{dataset} por completo.

En el mismo paper, el autor comenta: \emph{"We take an entirely data-driven approach"}. Por lo tanto, esto junto a nuestra falta de conocimiento sobre el problema, justifica que usemos ténicas estadísticas para establecer el conjunto de características a emplear (principalmente \emph{PCA}), y una ténica como \emph{cross-validation} para seleccionar el modelo a emplear, las transformaciones sobre los datos y distintos parámetros referentes al modelo escogido.

\pagebreak

\subsubsection{Problema a resolver}

Queremos aprender una función objetivo de la forma:

$$f: \mathcal{X} \rightarrow \mathcal{Y}$$

donde $\mathcal{X}$ es el conjunto real de dimensión 81 (las 81 características de las que disponemos), e $\mathcal{Y}$ son valores reales, en el intervalo $[0, \infty]$. Como comentaremos en \emph{\ref{descripcion_caracteristicas}. \nameref{descripcion_caracteristicas}}, la unidad de medida de la temperatura son los Kelvin, y por tanto, tenemos una cota inferior de esta variable real.

Más adelante realizaremos transformaciones sobre el conjunto de datos original, por lo tanto, pasaremos de aprender un $f: \mathcal{X} \rightarrow \mathcal{Y}$ a aprender un $f: \hat{\mathcal{X}} \rightarrow \mathcal{Y}$, donde $\hat{\mathcal{X}}$ tendra otra dimensión.

Por tanto quedan claros los elementos de un problema de regresión, queremos encontrar una función $g: \hat{\mathcal{X}} \rightarrow \mathcal{Y}$ de forma que $\forall x \in \hat{\mathcal{X}}, g(x) \approx f(x)$.

\subsubsection{Descripción de las caracteríticas} \label{descripcion_caracteristicas}

De nuevo, en el paper original \cite{original_paper_reg:paper} se describe el proceso de extracción de características, que pasamos a resumir brevemente.

Se parte de las siguientes propiedades de los átomos que componen las moléculas de los superconductores:

\begin{table}[H]
\begin{tabularx}{\textwidth}{|X|X|}
    \textbf{Variable} & \textbf{Descripción} \\
    \hline
    Masa atómica & Masa total del protón y neutrón en reposo \\
    Energía de primera ionización & Energia necesaria para eliminar una valencia del electrón \\
    Radio Atómico & Radio atómico \\
    Densidad & Densidad a una temperatura y presión estandar \\
    Afinidad del electrón & Energia necesaria para añadir un electrón a un átomo neutro \\
    Calor de fusión & Energía necesaria para pasar de estado sólido a líquido sin cambio de temperatura \\
    Conductividad térmica &  Coeficientes de conductividad térmica $\kappa$ \\
    Valencia & Número típico de enlaces químicos formados por el elemento \\
    \hline
\end{tabularx}
\caption{Propiedades de los elementos usadas para crear las \emph{features} }
\end{table}

Las \emph{features} más importantes a la hora de predecir $T_c$ son aquellas basadas en la \emph{thermal conductivity}, \emph{atomic radius}, \emph{valence}, \emph{electron affinity}, y \emph{atomic mass} \cite{original_paper_reg:paper}. Con esto podríamos pensar en descartar el resto de \emph{features} que no se basen en las anteriores, sin embargo, no tenemos el conocimiento suficiente sobre el problema para realizar este descarte con confianza, delegando esta decisión a la técnica \emph{Principal Componente Analysis} que más adelante desarrollaremos.

A partir de esto, se calculan las siguientes \emph{features}. Por cada material, en base a sus moléculas, se calculan los siguientes estadísticos por cada \emph{feature} mostrada en la anterior tabla y por cada átomo de la molécula:


\begin{itemize}
    \item Media
    \item Media ponderada
    \item Media geométrica
    \item Media geométrica ponderada
    \item Entropía
    \item Entropía ponderada
    \item Rango
    \item Rango ponderado
    \item Desviación estandar
    \item Desviación estandar ponderada
\end{itemize}

Algo importante a destacar es que la unidad de temperatura para $T_c$ es \emph{Kelvin}, por lo que esta variable estará acotada inferiormente por cero. La temperatura ambiente en kelvin está en torno a los $298K$, por lo tanto, valores cercanos a esta referencia serán los más interesantes a la hora de escoger o no un material como superconductor.

Las fórmulas para cada estadístico se pueden consultar en el ya mencionado \emph{paper} \cite{original_paper_reg:paper}. Notar que tenemos siempre el estadístico y su versión ponderada.

Teniendo 8 variables, y 10 estadísticas por cada variable, llegamos a 80 características en el dataset. La característica que falta para llegar a las 81, es el número de elementos que compone la molécula del superconductor.

\subsubsection{Exploración del \emph{Dataset}}

Antes de empezar a explorar los datos del problema, separamos el conjunto de \emph{training} y de \emph{test}. No queremos saber nada sobre el \emph{test\_dataset} durante esta exploración de los datos, para evitar caer en el \emph{data snooping}. Esta separación de los datos la realizamos con la función \lstinline{split_data} en la que hacemos:

\begin{lstlisting}[language=Python]
    df_test, df_train = train_test_split(df, test_size = test_percentage, shuffle = True, stratify = None)
\end{lstlisting}

Notar que estamos mezclando los datos pues \lstinline{shuffle = True}, con ello, y teniendo en cuenta que disponemos de muchos datos, queremos tener una muestra de entrenamiento representativa de los datos. Por ejemplo, no queremos que tengamos desbalanceos en la variable de salida, es decir, que en \emph{train} tengamos filas con valores de $T_c$ bajo, mientras que en \emph{test} tengamos filas con valores de $T_c$ altos, o viceversa. Como no tenemos clases, y al tener una cantidad tan grande de datos, no hacemos \emph{stratify != None}. Confiamos en que la mezcla aleatoria haga que nuestras muestras sean representativas y balanceadas, en el sentido que ya se ha especificado.

Partimos de un \emph{dataset} con 21263 ejemplos. Al separar en \emph{train} y \emph{test}, nos quedamos con 17010 y 4253 ejemplos, respectivamente.

Con la función \lstinline{explore_training_set} hacemos una pequeña exploración estadística de los datos, en la que mostramos una tabla con las estadísticas de las columnas. Esta tabla es la correspondiente a: \emph{\ref{Tabla con los estadísticos de las features}. \nameref{Tabla con los estadísticos de las features}}.


% TODO -- poner aqui los datos, no se como hacerlo de forma limpia
\begin{table}
\begin{tabularx}{\textwidth}{|X|X|X|X|X|X|X|X|X|X|X|}
name& type& mean& median& var& sdt& min& max& p25& p75& missing vals \\
\hline
    \end{tabularx}
    \caption{Estadísticas de las \emph{features} del conjunto de entrenamiento}
    \label{Tabla con los estadísticos de las features}
\end{table}








==> Mostramos las estadisticas de las variables sin normalizar => tabla que he construido con pandas
==> Rangos de variables dispares => justifica la necesidad de normalizar los datos
==> Todos los datos son reales o enteros => no es necesario realizar otro tipo de codificaciones
==> Mostramos el boxplot de la variable de salida => Problema con la representación de la muestra. Muestra algo sesgada, pocos ejemplos en la muestra para valores de temperatura altos. Mostrar la figure 4 del paper. Pocos superconductores de alto Tc que son precisamente los que interesan
==> Mostrar tabla de correlaciones???

\subsection{Preprocesado de los datos}

\subsubsection{Eliminación de outliers}

==> Eliminamos outliers => valores que se desvían más de 4 veces la desviación típica
==> Eliminamos outliers antes de normalizar, porque estamos normalizando con la media y la desviacion tipica, los outliers pueden afectar a una buena normalizacion: https://scikit-learn.org/stable/modules/preprocessing.html\#scaling-data-with-outliers => esta en el .bib
==> Outliers con una sola variable por simplicidad => tenemos muchos datos, la pérdida no va a ser muy grande
==> Decir lo que es el Z-score (numero de desviaciones típicas en las que nos alejamos de la media). Quitamos filas con z-score mayor que 4
==> Si suponemos una distribucion normal => 99.74\% de los datos en [-3 std, 3 std]
==> Quitamos el 8.5\% de los datos. En el paper se cargan el 40\% de los datos, así que parece buena idea eliminar estos datos
==> Mostramos de nuevo la variable de salida => sigue funcionando bien porque está toda la variabilidad
==> El eliminado de los outliers es clave para el procedimiento PCA: https://medium.com/apprentice-journal/pca-application-in-machine-learning-4827c07a61db => el enlace esta en la .bib
==> Mostramos por pantalla los valores de salida que eliminamos por culpa de outliers -> Eliminamos valores bajos asi que no preocuparse -> Además mostrar la tabla,

\subsubsection{Principal Component Analysis}

==> Como se muestra en https://medium.com/apprentice-journal/pca-application-in-machine-learning-4827c07a61db (pillar el bib) está muy afectado por outliers, que ya nos hemos preocupado de eliminar
=> Con 10 componentes obtenemos muchisima varianza explicada -> variables muy correladas, son estadisticas de lo mismo
=> 8 variables originales -> 10 parece suficiente
=> Poner el porcentaje de la variable explicada
        => 0.9995126576534428
==> No normalizamos antes porque entonces PCA funciona peor

\subsubsection{Normalización}

==> Ya hemos justificado la necesidad de aplicar normalizacion
==> Aplicamos normalización bien => sin tener en cuenta información del test => mostrar el código porque puede ser relevante
==> Comentar cómo estamos usando el scaler de sklearn
==> La variable de salida no se normaliza, pues esto no tiene sentido
==> Mostar de nuevo la tabla con los datos

\subsection{Selección del modelo}

\subsubsection{Selección de la métrica de error}

=> Métrica de error -> error cuadrático medio
=> No uso error medio -> con error cuadrático medio se castigan más los outliers. Son funciones con una derivada mas sencilla a la hora de minimizar
=> El ECM es de los pocos errores que hemos visto a la hora de aplicar regresión, mientras que en clasificación teníamos más variedad para elegir
=> No usamos SGDRegressor porque tenemos la suficiente potencia de cómputo para usar Lasso, Ridge y LinearModel. Cuando el ordenador no puede más, es por la transformacion polinomica
=> Asi que elijo Linear normal, ridge y Lasso para CV etapa1
    => Laso es interesante porque favorece que muchos coeficientes sean cero. Ridge favorece pesos pequeños pero no cero. Interesante para quitar terminos del polynomial fit que no me interesan

\subsubsection{Modelos candidatos}

A la hora de seleccionar modelos disponemos de:

\begin{itemize}
    \item Ajustar un hiperplano
\end{itemize}

Así que en realidad a la hora de escoger un modelo debemos escoger la trasnsformación de los datos sobre la que ajustaremos un hiperparámetro. También deberemos escoger el regularizador

\subsubsection{\emph{Cross-Validation}, primera etapa}

=> Explicar que he hecho en codigo, seleccion de lambda fijado por mi, la metrica de error negativa, las transformaciones que hemos realizado...
=> Comentar que en lasso saltan errores, y se vuelve a intentar asignando distintos valores o mas iteraciones -> No estoy muy seguro de esta politica
=> Cuando uso No PCA, antes hago una normalizacion de los datos

Los resultados de \emph{Cross Validation} se resumen en la siguiente tabla:

\begin{table}[H]
\begin{tabularx}{\textwidth}{|X|X|X|X|X|X|}
    \hline
    \textbf{Modelo} & \textbf{PCA \/ No PCA}& \textbf{Orden de la transformación polinómica} & \textbf{Valor medio} & \textbf{Valor mínimo} & \textbf{Valor máximo} \\
    \hline
    Lineal & PCA & 1 & -572.19 & -601.20 & -535.89 \\
    Lineal & PCA & 2 & -431.65 &-476.37 & -412.32 \\
    Lineal & PCA & 3 &-350.63 &-386.93 & -326.35 \\
    Lineal & PCA & 4 & -2470.37 &-15358.09 & -283.22 \\
    Ridge  & PCA & 1 &-572.15 &-609.16 & -541.24 \\
    Ridge  & PCA & 2 & -431.82 &-472.09 & -404.95 \\
    Ridge  & PCA & 3 & -348.22 &-363.54 & -332.83 \\
    Ridge  & PCA & 4 & -2126.95 &-14843.43 & -333.92 \\
    Lasso  & PCA & 1 & -572.15 &-634.92 & -549.44 \\
    Lasso  & PCA & 2 & -432.08 &-459.59 & -413.28 \\
    Lasso  & PCA & 3 & -358.46 &-382.28 & -335.32 \\
    Lasso  & PCA & 4 & -361.25 &-592.61 & -304.22 \\
    Lineal & No PCA & 1 & -310.50 &-322.55 & -282.40 \\
    Ridge  & No PCA & 1 & -310.77 &-335.69 & -292.88 \\
    Lasso  & No PCA & 1 & -328.42 &-355.69 & -310.94 \\
    \hline
\end{tabularx}
    \caption{Resultados de \emph{Cross Validation}, primera fase}
\end{table}

=> Por tanto, escogemos no aplicar PCA
=> Por tanto, aunque la mejor media la da Linea No PCA, usamos Ridge No PCA, pues difieren solo en media en 0.27 ECM y no hemos jugado con el parametro de regularizacion, asi que parece que vamos a poder mejorar aun mas el performance del modelo
    -> Ademas, ridge es más estable a vista del Maximo Minimo en un intervalo de menor longitud
=> Puede tener sentido no aplicar PCA -> $N \geq d_{VC} * 10 \rightarrow d_{VC} \leq 1600$

\subsubsection{\emph{Cross-Validation, segunda etapa}}

=> Elegimos el dataset original sin PCA ni transformaciones polinomicas
=> Regularizador ridge
=> Rango de valores para lambda: $[10^-6 ... 10^-1, 1, 2]$ -> basado en ??

=> Se resume en la siguiente tabla:

\begin{table}[H]
\begin{tabularx}{\textwidth}{|X|X|X|X|}
    \hline
    \textbf{Lambda} & \textbf{Valor medio} & \textbf{Valor mínimo} & \textbf{Valor máximo} \\
    \hline
    1e-07 & -310.50 &  -322.55 & -282.40 \\
    1e-06 & -310.50 &  -322.55 & -282.40 \\
    5e-06 & -310.55 &  -335.61 & -272.69 \\
    1e-05 & -310.77 &  -335.67 & -292.98 \\
    0.0001 & -310.74 &  -343.24 & -291.18 \\
    0.001 & -310.79 &  -348.74 & -287.12 \\
    0.01 & -310.58 &  -356.06 & -283.69 \\
    0.1 & -310.75 &  -324.71 & -298.66 \\
    1 & -310.99 &  -335.25 & -276.35 \\
    2 & -311.13 &  -329.18 & -291.28 \\
    5 & -311.90 &  -336.92 & -287.86 \\
    \hline
\end{tabularx}
    \caption{Resultados de \emph{Cross Validation}, segunda fase}
\end{table}

=> A vista de esto, usamos $\lambda = 10^{-7}$


==> TODO -- en algun commento comentar que estamos usando unos baselines
\pagebreak
\section{Problema de clasificación}

\pagebreak

% Bibliografia
\bibliography{./References}
\bibliographystyle{ieeetr}

\end{document}
